{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Final Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Mateo Ignacio\n",
    "- Samuel Piltch\n",
    "- Nate del Rosario üêê\n",
    "- Lisa Hwang\n",
    "- Geovaunii D. White\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we‚Äôve never worked with audio data or classification of audio data we wanted to try working with data that is unstructured as such. We ask the question: how do the audio features from wav files of cat and dog sounds compare to each other, and can we use these features to classify animal noises based on converting these wav files to numeric features?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we will be working to create an algorithm that can classify animal noises. Although with the average human's capabilities, it should be easy to discern the difference between a cat's meow and a dog's bark, this is not as simple to program with technology. After some initial research, there are differences in a dog's bark and a cat's meow that can be distinguished by a machine. This is not a question that is unique, as there have been many studies and models that have been trained in a similar capacity. \n",
    "\n",
    "For example, there is a study in which scientists created a neural network that has the ability to classify animal sounds. The algorithm was built with data consisting of recordings of marmoset monkeys in a loud environment. Their network is able to identify the animal that made the noise as well as the call type, by using spectrogram images [^1]. In another study from 2020, an algorithm is able to classify an animal sound from its dissimilarity to another animal sound. The classifier system was produced with Siamese neural networks (SNNs) and support vector machines (SVMs) to identify dissimilarity spaces of spectrograms [^2]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[^1]: Tuomas Oikarinen, Karthik Srinivasan, Olivia Meisner, Julia B. Hyman, Shivangi Parmar, Adrian Fanucci-Kiss, Robert Desimone, Rogier Landman, Guoping Feng; Deep convolutional network for animal sound classification and source attribution using dual audio recordings. J Acoust Soc Am 1 February 2019; 145 (2): 654‚Äì662. https://doi.org/10.1121/1.5087827\n",
    "\n",
    "[^2]: Nanni L, Brahnam S, Lumini A, Maguolo G. Animal Sound Classification Using Dissimilarity Spaces. Applied Sciences. 2020; 10(23):8578. https://doi.org/10.3390/app10238578"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain audio features will be statistically different between the distribution of cats and dogs. For example, the mean \"pitch\" of cat wav files will be higher than that of dogs since cats generally have higher voices, while the mean \"audio level / HZ\" of dog wav files will be higher than that of cats since dogs are generally louder animals in comparison to cats. \n",
    "If numerical data is extracted from the audio then models can be trained to predict the species making the noise based off of these metrics since there will be enough difference between certain features between the two. This approach of comparing audio features between two groups can then be applied to other groups of living beings/objects to classify the two.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Variables we would ideally have are audio length, decibels (how loud), species classification as a response variable.  (supervised dataset?)In the best situation the .wav files would be accompanied by a csv or json of numerical already preprocessed so we would not have to clean any of the data. For audio a dataset that is sufficiently large require less observations than necessary for other models since we would have to account for memory issues and such so around 2000 or more observations would be ideal. The observations would be sounds dogs and cats make, like meows, barks and howls. Again memory is theoretically an issue since long audio files could create noise and issues with memory as typically large files are not necessary and a compact csv or json is all that is needed.\n",
    "2. We have found a dataset through Kaggle that is a sample from a larger dataset of sounds including planes, humans, and traffic. This data is already readily available and it includes .wav files. Since there are no accompanying csv files we must make the table of variables ourselves which may prove challenging. The preprocessing will probably take the most time. This data is from the AE-Dataset creator which itself was made from FreeSound. Naoya Takahashi, Michael Gygli, Beat Pfister and Luc Van Gool, \"Deep Convolutional Neural Networks and Data Augmentation for Acoustic Event Recognition\", Proc. Interspeech 2016, San Fransisco."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ethical concerns in the industry of music streaming, such as the income of musicians (especially independent artists).\n",
    "\n",
    "Spotify's Code of Conduct is posted publicly [^3], where they claim that they comply with laws and are fully fair and in line with their interests. Additionally, they provide many links for contact if any individual suspects fraud or theft of Spotify assets. Spotify has a separate Privacy Policy, where they go into detail of the personal rights of users [^4]. \n",
    "\n",
    "There is potential for bias in the Spotify weekly charts, as popular artists are more likely to have a significant following, leading to an overrepresentation in the charts. Therefore, independent and/or lesser known artists do not have the same opportunities. Promotion from record labels who can afford to advertise their artists/songs would have the upper hand in this field. Additionally, we do not know the specifics of the algorithms used to generate Spotify's charts, which would potentially harbor bias in its model. \n",
    "\n",
    "[^3]: https://s29.q4cdn.com/175625835/files/doc_downloads/gov-docs/Spotify-Code-of-Conduct-and-Ethics-(Updated-2020).pdf \n",
    "\n",
    "[^4]: https://www.spotify.com/us/legal/privacy-policy/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "* Everyone should contribute something, and no one or few people should contribute too much to allow others to add their own contributions.\n",
    "\n",
    "* When one person writes code or makes a change to a part of the project, it should be reviewed by at least one other group member (preferably most or all)\n",
    "\n",
    "* Code Review SHOULD be done through pull requests \n",
    "\n",
    "* Every member will create a branch and make their changes there for version control\n",
    "\n",
    "* We will communicate through text and on call (when free)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 5/10  |  6 PM | Read & Think about COGS 108 expectations; begin discussing and implementing Data Wrangling  | Go over shortcomings and successes, check that the data is in the correct format we want | \n",
    "| 5/17  |  6 PM |  Finalize data cleaning, wrangling, storage | Begin brainstorming EDA methods | \n",
    "| 5/24  | 6 PM  | Iterate  | Discuss  possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 5/31  | 6 PM  | Finish EDA and begin model selection | Assign Tickets  |\n",
    "| 6/7   | 6 PM  | Code Review and begin implementing and testing | AGILE and go over findings |\n",
    "| 6/14  | 12 PM  | Complete analysis; Draft results/conclusion/discussion (Wasp)| Discuss/edit full project |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (default, Apr 24 2021, 22:26:35) \n[Clang 12.0.0 (clang-1200.0.32.29)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0c5cb6bf509a02cef9c56bf2eb97e86fe98bb4322bfa6beb984e4c134a4af93"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
